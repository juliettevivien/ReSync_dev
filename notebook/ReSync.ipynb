{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 1 : Import necessary functions, set parameters and load your own recordings ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import all the librairies and functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of librairies and functions\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python sys 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:14:58) [MSC v.1929 64 bit (AMD64)]\n",
      "pandas 1.5.3\n",
      "numpy 1.23.5\n",
      "mne 1.3.0\n",
      "sci-py 1.10.0\n",
      "matplotlib 3.6.3\n"
     ]
    }
   ],
   "source": [
    "# check some package versions for documentation and reproducibility\n",
    "import sys\n",
    "import mne\n",
    "from matplotlib import __version__ as plt_version\n",
    "import scipy\n",
    "print('Python sys', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "print('mne', mne.__version__)\n",
    "print('sci-py', scipy.__version__)\n",
    "print('matplotlib', plt_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cd_repo_folder():\n",
    "    \"\"\"sets current working directory to main repo folder\"\"\"\n",
    "    cd = os.getcwd()\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    while os.path.basename(cd) != 'ReSync':\n",
    "\n",
    "        cd = os.path.dirname(cd)\n",
    "        check += 1\n",
    "        if check > 10: raise ValueError('Repo path not found')\n",
    "    \n",
    "    os.chdir(cd)\n",
    "\n",
    "    print(f'working directory changed to {os.getcwd()}')\n",
    "\n",
    "    return os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working directory changed to c:\\Users\\Juliette\\Research\\Projects\\Synchronization_project\\Code\\ReSync\n"
     ]
    }
   ],
   "source": [
    "project_path = set_cd_repo_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom-made functions\n",
    "import functions.preprocessing as preproc\n",
    "import functions.utils as utils\n",
    "import functions.plotting as plot\n",
    "import functions.find_artefacts as artefact\n",
    "import functions.crop as crop\n",
    "import functions.main_resync as resync\n",
    "#import functions.plotting_interactive as plot_interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions.main_resync' from 'c:\\\\Users\\\\Juliette\\\\Research\\\\Projects\\\\Synchronization_project\\\\Code\\\\ReSync\\\\functions\\\\main_resync.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(plot)\n",
    "importlib.reload(preproc)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(artefact)\n",
    "importlib.reload(crop)\n",
    "importlib.reload(resync)\n",
    "#importlib.reload(plot_interact)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load your own LFP data:**\n",
    "\n",
    "Resulting variables needed for subsequent analysis:\n",
    "- LFP_array (np.ndarray, 6d): the LFP recording which has to be aligned, containing all channels\n",
    "- lfp_sig (np.ndarray, 1d): the channel containing the LFP signal from the hemisphere where the stimulation was delivered to create artefacts\n",
    "- LFP_rec_ch_names (list): names of all the channels, in a list (will be used to annotate cropped recording)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pyPerceive functions\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.chdir(os.path.join(os.getcwd(), 'PyPerceive'))\n",
    "os.chdir(os.path.join(os.getcwd(), 'code'))\n",
    "pyPerceive_path = os.getcwd()\n",
    "print (f'working dir to go fetch PyPerceive functions:{pyPerceive_path}')\n",
    "\n",
    "from PerceiveImport.classes import (\n",
    "    main_class, modality_class, metadata_class,\n",
    "    session_class, condition_class, task_class,\n",
    "    contact_class, run_class\n",
    ")\n",
    "import PerceiveImport.methods.load_rawfile as load_rawfile\n",
    "import PerceiveImport.methods.find_folders as find_folders\n",
    "import PerceiveImport.methods.metadata_helpers as metaHelpers\n",
    "\n",
    "#reset the proper working directory for the analysis\n",
    "os.chdir(project_path)\n",
    "print (f'working dir set back to:{project_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose LFP file\n",
    "sub041 = main_class.PerceiveData(\n",
    "    sub = \"041\", \n",
    "    incl_modalities=['streaming'],\n",
    "    incl_session = [\"fu12m\"],\n",
    "    incl_condition =['m0s0','m0s1','m1s0','m1s1'],\n",
    "    incl_task = [\"rest\",\"fingerTap\"],\n",
    "    # incl_contact = [\"RingL\", \"SegmInterR\", \"SegmIntraR\"],\n",
    "    import_json=False,\n",
    "    warn_for_metaNaNs=True,\n",
    "    allow_NaNs_in_metadata=False\n",
    ")\n",
    "\n",
    "# define LFP data\n",
    "LFP_rec = sub041.streaming.fu12m.m1s0.fingerTap.run1.data\n",
    "LFP_array = LFP_rec.get_data()\n",
    "ch_i = 0 #choose index of the channel containing the stim artefacts (O for left hemisphere, 1 for right hemisphere)\n",
    "lfp_sig = LFP_rec.get_data()[ch_i]\n",
    "LFP_rec_ch_names = LFP_rec.ch_names\n",
    "\n",
    "n_chan = len(LFP_rec.ch_names)\n",
    "time_duration_LFP = (LFP_rec.n_times/LFP_rec.info['sfreq']).astype(float)\n",
    "print(     \n",
    "\tf'The data object has:\\n\\t{LFP_rec.n_times} time samples,'      \n",
    "\tf'\\n\\tand a sample frequency of {LFP_rec.info[\"sfreq\"]} Hz'      \n",
    "\tf'\\n\\twith a recording duration of {time_duration_LFP} seconds.'      \n",
    "\tf'\\n\\t{n_chan} channels were labeled as \\n{LFP_rec.ch_names}.')\n",
    "print(f'The channel containing artefacts has index {ch_i} and is named {LFP_rec.ch_names[ch_i]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load your own external data:**\n",
    "(our external data recorder is a TMSi Data recorder.)\n",
    "\n",
    "PM: NOTICE THE POP UP WINDOW AFTER RUNNING, TO SELECT THE FILE LOCATION\n",
    "\n",
    "Resulting variables:\n",
    "- external_file (np.ndarray, multi-dimensional): the complete external recording containing all channels recorded\n",
    "- BIP_channel (np.ndarray, 1d): the channel containing the signal from the bipolar electrode used to pick up the artefacts on the IPG/cable\n",
    "- external_rec_ch_names (list, same length as the number of channels in external_file): list of the channels names, to rename them accordingly after alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions.tmsi_poly5reader as poly5_reader\n",
    "import functions.loading_TMSi as loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMSi_data = poly5_reader.Poly5Reader()  # open TMSi data from poly5\n",
    "(BIP_channel,\n",
    " external_file,\n",
    " external_rec_ch_names) = loading.load_TMSi_artefact_channel(TMSi_data) # function adapted for our own data recorder, \n",
    "# to load all necessary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READ ME**\n",
    "\n",
    "Before starting the run_resync function, be careful to check that the config file is properly set. In particular, pay attention to:\n",
    "- write the proper subject ID (to not overwrite previous analysis)\n",
    "- write the correct sampling frequencies corresponding to YOUR recordings\n",
    "- by default use kernel \"2\", and set \"real_index_LFP\" to 0 for the first run. This can be adjusted if necessary before re-running.\n",
    "- by default, set \"consider_first_seconds_LFP\", \"consider_first_seconds_external\" and \"ignore_first_seconds_external\" to null. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters to adjust for our systems are:\n",
    "\n",
    "- For TMSi SAGA with sf = 4000Hz or 4096Hz : thresh_external = -0.001, ch_name_BIP = \"BIP 01\"\n",
    "- For TMSi Porti with sf = 2048Hz : thresh_external = -2000, ch_name_BIP = \"Bip25\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 2: Align recordings: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LFP_df_offset, \n",
    " external_df_offset) = resync.run_resync(\n",
    "    LFP_array=LFP_array,\n",
    "    lfp_sig=lfp_sig,\n",
    "    LFP_rec_ch_names=LFP_rec_ch_names,\n",
    "    external_file=external_file,\n",
    "    BIP_channel=BIP_channel,\n",
    "    external_rec_ch_names=external_rec_ch_names,\n",
    "    SHOW_FIGURES = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 3 : Look for timeshift ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(resync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resync.ecg(LFP_df_offset,\n",
    "    external_df_offset,\n",
    "    SHOW_FIGURES = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(resync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resync.run_timeshift_analysis(\n",
    "    LFP_df_offset,\n",
    "    external_df_offset,\n",
    "    SHOW_FIGURES = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#import settings\n",
    "json_path = os.path.join(os.getcwd(), 'config')\n",
    "json_filename = 'config.json'  # dont forget json extension\n",
    "with open(os.path.join(json_path, json_filename), 'r') as f:\n",
    "    loaded_dict =  json.load(f)\n",
    "\n",
    "#set saving path\n",
    "if loaded_dict['saving_path'] == False:\n",
    "    saving_path = utils.define_folders()\n",
    "else:\n",
    "    saving_path = os.path.join(os.path.normpath(loaded_dict['saving_path']), loaded_dict['subject_ID'])\n",
    "    if not os.path.isdir(saving_path):\n",
    "        os.makedirs(saving_path)\n",
    "\n",
    "\n",
    "### DETECT ARTEFACTS ###\n",
    "\n",
    "# Reselect artefact channels in the aligned (= cropped) files\n",
    "LFP_channel_offset = LFP_df_offset.iloc[:,loaded_dict['LFP_ch_index']].to_numpy()  \n",
    "BIP_channel_offset = external_df_offset.iloc[:,loaded_dict['BIP_ch_index']].to_numpy() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_external_offset = preproc.filtering(BIP_channel_offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new timescales:\n",
    "LFP_timescale_offset_s = np.arange(0,(len(LFP_channel_offset)/loaded_dict['sf_LFP']),1/loaded_dict['sf_LFP'])\n",
    "external_timescale_offset_s = np.arange(0,(len(BIP_channel_offset)/loaded_dict['sf_external']),1/loaded_dict['sf_external'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 8: Both signals aligned with all their artefacts detected:\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "fig.suptitle(str(loaded_dict['subject_ID']))\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(12)\n",
    "ax1.axes.xaxis.set_ticklabels([])\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Intracerebral LFP channel (µV)')\n",
    "ax2.set_ylabel('External bipolar channel (mV)')\n",
    "#ax1.set_xlim(0,len(LFP_channel_offset)/loaded_dict['sf_LFP']) \n",
    "#ax2.set_xlim(0,len(LFP_channel_offset)/loaded_dict['sf_LFP']) \n",
    "ax1.set_xlim(264.9,265.4) \n",
    "ax2.set_xlim(264.9,265.4)\n",
    "ax1.set_ylim(-500,500)\n",
    "ax1.scatter(LFP_timescale_offset_s,LFP_channel_offset,color='darkorange',zorder=1, s=3)\n",
    "ax2.plot(external_timescale_offset_s,filtered_external_offset, color='darkcyan',zorder=1, linewidth=0.1) \n",
    "\n",
    "fig.savefig(saving_path + '\\\\last artefact zoomed.png',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(LFP_timescale_offset_s,LFP_channel_offset,color='darkorange',zorder=1, s=5)\n",
    "plt.title(str(loaded_dict['subject_ID']))\n",
    "plt.xlim(265,265.4)\n",
    "plt.ylim(-400,200)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Intracerebral LFP channel (µV)')\n",
    "plt.savefig(saving_path + '\\\\last artefact zoomed2.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet loss analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "1\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "2\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "3\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "4\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "5\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "6\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "7\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "8\n",
      "LFP Data is missing!!\n",
      "9\n",
      "LFP Data is missing!!\n",
      "10\n",
      "LFP Data is missing!!\n",
      "11\n",
      "LFP Data is missing!!\n",
      "12\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "13\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "14\n",
      "LFP Data is missing!!\n",
      "15\n",
      "LFP Data is missing!!\n",
      "16\n",
      "No LFP data missing based on timestamp differences between data-packets\n",
      "17\n",
      "No LFP data missing based on timestamp differences between data-packets\n"
     ]
    }
   ],
   "source": [
    "json_fname = 'Report_Json_Session_Report_20230523T113952.json'  ## write json filename with extension\n",
    "sub = '047'\n",
    "resync.check_packet_loss(json_fname,sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d88ae3f494ed6750ac12c087146162cb95510d20525109b4b9e2bae86cf73368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
